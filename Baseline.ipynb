{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./labels.csv\")\n",
    "imgs = [\"{}.jpg\".format(x) for x in list(data.id)]\n",
    "img_label = list(data['breed'])\n",
    "ndata = pd.DataFrame({'id': imgs,'breed': img_label})\n",
    "ndata.breed = ndata.breed.astype(str)\n",
    "spndata = np.split(ndata, [9199], axis=0)\n",
    "train_data = spndata[0]\n",
    "val_data = spndata[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "TRAIN_PATH = '../img/train/'\n",
    "TEST_PATH = '../img/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(rescale=1./255)\n",
    "val_image_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9199 validated image filenames belonging to 120 classes.\n",
      "Found 1023 validated image filenames belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = train_image_generator.flow_from_dataframe(\n",
    "                                        dataframe = train_data,\n",
    "                                        directory=TRAIN_PATH,x_col=\"id\",\n",
    "                                        y_col=\"breed\",\n",
    "                                        class_mode=\"categorical\",\n",
    "                                        target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True)\n",
    "val_data_gen = val_image_generator.flow_from_dataframe(\n",
    "                                        dataframe = val_data,\n",
    "                                        directory=TRAIN_PATH,x_col=\"id\",\n",
    "                                        y_col=\"breed\",\n",
    "                                        class_mode=\"categorical\",\n",
    "                                        target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                        batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define you model here\n",
    "def get_base_model():\n",
    "    input1 = Input(shape=(150, 150, 3))\n",
    "    x = Conv2D(20, kernel_size=3, activation='relu', input_shape=(150,150,3))(input1)\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Conv2D(40, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(400, activation='relu')(x)\n",
    "    out = Dense(120, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "                  metrics=['categorical_crossentropy','categorical_accuracy','accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = get_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 148, 148, 20)      560       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 40)        7240      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 40)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 51840)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 400)               20736400  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               48120     \n",
      "=================================================================\n",
      "Total params: 20,792,320\n",
      "Trainable params: 20,792,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Total parmas should not exceed 120M or the VM may crash\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Epoch 1/10\n",
      "8/9 [=========================>....] - ETA: 9s - loss: 6.2862 - categorical_crossentropy: 6.2861 - categorical_accuracy: 0.0094 - accuracy: 0.0094 \n",
      "Epoch 00001: val_loss improved from inf to 4.80280, saving model to model_base.h5\n",
      "9/9 [==============================] - 96s 11s/step - loss: 6.1261 - categorical_crossentropy: 6.1257 - categorical_accuracy: 0.0090 - accuracy: 0.0090 - val_loss: 4.8028 - val_categorical_crossentropy: 4.8028 - val_categorical_accuracy: 0.0088 - val_accuracy: 0.0088\n",
      "Epoch 2/10\n",
      "8/9 [=========================>....] - ETA: 2s - loss: 4.7907 - categorical_crossentropy: 4.7907 - categorical_accuracy: 0.0114 - accuracy: 0.0114\n",
      "Epoch 00002: val_loss improved from 4.80280 to 4.78545, saving model to model_base.h5\n",
      "9/9 [==============================] - 33s 4s/step - loss: 4.7900 - categorical_crossentropy: 4.7900 - categorical_accuracy: 0.0124 - accuracy: 0.0124 - val_loss: 4.7855 - val_categorical_crossentropy: 4.7855 - val_categorical_accuracy: 0.0137 - val_accuracy: 0.0137\n",
      "Epoch 3/10\n",
      "8/9 [=========================>....] - ETA: 3s - loss: 4.7742 - categorical_crossentropy: 4.7742 - categorical_accuracy: 0.0169 - accuracy: 0.0169\n",
      "Epoch 00003: val_loss improved from 4.78545 to 4.76427, saving model to model_base.h5\n",
      "9/9 [==============================] - 36s 4s/step - loss: 4.7733 - categorical_crossentropy: 4.7733 - categorical_accuracy: 0.0171 - accuracy: 0.0171 - val_loss: 4.7643 - val_categorical_crossentropy: 4.7643 - val_categorical_accuracy: 0.0147 - val_accuracy: 0.0147\n",
      "Epoch 4/10\n",
      "8/9 [=========================>....] - ETA: 3s - loss: 4.7025 - categorical_crossentropy: 4.7025 - categorical_accuracy: 0.0300 - accuracy: 0.0300\n",
      "Epoch 00004: val_loss improved from 4.76427 to 4.71281, saving model to model_base.h5\n",
      "9/9 [==============================] - 37s 4s/step - loss: 4.7002 - categorical_crossentropy: 4.7002 - categorical_accuracy: 0.0301 - accuracy: 0.0301 - val_loss: 4.7128 - val_categorical_crossentropy: 4.7128 - val_categorical_accuracy: 0.0196 - val_accuracy: 0.0196\n",
      "Epoch 5/10\n",
      "8/9 [=========================>....] - ETA: 3s - loss: 4.5234 - categorical_crossentropy: 4.5234 - categorical_accuracy: 0.0616 - accuracy: 0.0616\n",
      "Epoch 00005: val_loss improved from 4.71281 to 4.64529, saving model to model_base.h5\n",
      "9/9 [==============================] - 37s 4s/step - loss: 4.5128 - categorical_crossentropy: 4.5130 - categorical_accuracy: 0.0624 - accuracy: 0.0624 - val_loss: 4.6453 - val_categorical_crossentropy: 4.6453 - val_categorical_accuracy: 0.0235 - val_accuracy: 0.0235\n",
      "Epoch 6/10\n",
      "8/9 [=========================>....] - ETA: 3s - loss: 4.2104 - categorical_crossentropy: 4.2104 - categorical_accuracy: 0.1205 - accuracy: 0.1205\n",
      "Epoch 00006: val_loss improved from 4.64529 to 4.56248, saving model to model_base.h5\n",
      "9/9 [==============================] - 37s 4s/step - loss: 4.2020 - categorical_crossentropy: 4.2020 - categorical_accuracy: 0.1198 - accuracy: 0.1198 - val_loss: 4.5625 - val_categorical_crossentropy: 4.5625 - val_categorical_accuracy: 0.0391 - val_accuracy: 0.0391\n",
      "Epoch 7/10\n",
      "8/9 [=========================>....] - ETA: 3s - loss: 3.8153 - categorical_crossentropy: 3.8153 - categorical_accuracy: 0.1978 - accuracy: 0.1978\n",
      "Epoch 00007: val_loss did not improve from 4.56248\n",
      "9/9 [==============================] - 36s 4s/step - loss: 3.8032 - categorical_crossentropy: 3.8032 - categorical_accuracy: 0.1974 - accuracy: 0.1974 - val_loss: 4.6051 - val_categorical_crossentropy: 4.6051 - val_categorical_accuracy: 0.0518 - val_accuracy: 0.0518\n",
      "Epoch 8/10\n",
      "8/9 [=========================>....] - ETA: 3s - loss: 3.3333 - categorical_crossentropy: 3.3328 - categorical_accuracy: 0.2979 - accuracy: 0.2979\n",
      "Epoch 00008: val_loss did not improve from 4.56248\n",
      "9/9 [==============================] - 36s 4s/step - loss: 3.3102 - categorical_crossentropy: 3.3098 - categorical_accuracy: 0.2998 - accuracy: 0.2998 - val_loss: 4.6154 - val_categorical_crossentropy: 4.6154 - val_categorical_accuracy: 0.0538 - val_accuracy: 0.0538\n",
      "Epoch 9/10\n",
      "8/9 [=========================>....] - ETA: 3s - loss: 2.8176 - categorical_crossentropy: 2.8174 - categorical_accuracy: 0.4358 - accuracy: 0.4358\n",
      "Epoch 00009: val_loss did not improve from 4.56248\n",
      "9/9 [==============================] - 36s 4s/step - loss: 2.8107 - categorical_crossentropy: 2.8105 - categorical_accuracy: 0.4373 - accuracy: 0.4373 - val_loss: 4.6221 - val_categorical_crossentropy: 4.6221 - val_categorical_accuracy: 0.0528 - val_accuracy: 0.0528\n",
      "Epoch 10/10\n",
      "8/9 [=========================>....] - ETA: 3s - loss: 2.6119 - categorical_crossentropy: 2.6119 - categorical_accuracy: 0.4742 - accuracy: 0.4742\n",
      "Epoch 00010: val_loss did not improve from 4.56248\n",
      "9/9 [==============================] - 36s 4s/step - loss: 2.6065 - categorical_crossentropy: 2.6066 - categorical_accuracy: 0.4759 - accuracy: 0.4759 - val_loss: 4.6520 - val_categorical_crossentropy: 4.6520 - val_categorical_accuracy: 0.0567 - val_accuracy: 0.0567\n"
     ]
    }
   ],
   "source": [
    "# Path to save model parameters\n",
    "weight_path_model_base ='model_base.h5'\n",
    "# Path to write tensorboard\n",
    "tensorboard_path_model_base = 'Graphs/cnn_nn'\n",
    "\n",
    "callbacks_list_model_base = [\n",
    "    TensorBoard(log_dir=tensorboard_path_model_base, histogram_freq=1, write_graph=True, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "            weight_path_model_base,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "]\n",
    "\n",
    "history_base = base_model.fit_generator(train_data_gen, epochs=EPOCHS, verbose=1, validation_data=val_data_gen,\n",
    "                                        callbacks=callbacks_list_model_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load class details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = train_data_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverted_classes = dict(map(reversed, classes.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = [i for i in classes.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10357 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image_generator = ImageDataGenerator(rescale=1./255)\n",
    "test_data_gen = test_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                                                           directory=TEST_PATH,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 36s 3s/step\n"
     ]
    }
   ],
   "source": [
    "test_result = base_model.predict_generator(test_data_gen, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [i[i.find('/')+1:i.rfind('.')] for i in test_data_gen.filenames]\n",
    "filenames_arr = np.array(filenames)[np.newaxis].T\n",
    "test_result_with_label = np.concatenate((filenames_arr, test_result), axis=1)\n",
    "csv_header = ['id'] + class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be aware that the file with the same name should not exist, or it will be overwrited.\n",
    "result_dataframe = pd.DataFrame(test_result_with_label, columns=csv_header)\n",
    "result_dataframe.to_csv('./result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
